
## Try Example : 
Exercise 1
Transformers can be used for a variety of tasks. In the following code, we will implement sentiment analysis to classify whether a sentence has a positive, neutral, or negative meaning. Complete the code by following the instructions below:

Click the button below to launch the app. Once the app is launched, wait until the kernel starts. In the top-right corner, Python 3 (ipykernel) will show once the Notebook is ready to start. Run the cells inside the Jupyter Notebook and experiment by changing the text or the parameters in the Notebook.

Define the task as text classification.

Write the sentence to be tokenized and embedded.

Define the embedding by piping the sentence.

## quiz
1. An online platform automatically tags uploaded articles as Sports, Politics, or Fashion. What technology is being used here?
A. Text summarization
B. Entity extraction
C. Text classification
D. Speech-to-text conversion

2. An online platform extracts names, dates, and locations from news invoices. What technology is being used here?
A. Entity extraction
B. Text classification
C. Text-to-speech conversion
D. Machine translation

3. Which statement is true for the transformerâ€™s encoder-decoder architecture?
A. The encoder maps the input text to a vector representation, and the decoder translates this into an output sequence.
B. The encoder outputs the final prediction, and the decoder provides a probability distribution over the next possible word.
C. The encoder is only used during training, while the decoder is used during inference.
D. The decoder operates independently of the encoder and does not utilize its outputs.

4. What is the purpose of tokenization in the context of natural language processing and transformers?
A. To add semantic information to the tokens
B. To add the position of each token in the text
C. To divide the text into a list of tokens or words
**Tokenization is the process of converting text into words or tokens.**
D.To convert the tokens in the text to vectors


5. What does embedding represent in a transformer model?
A. The frequency of each token in the text
B. The grammatical category of each token
C. The position of each token in the sentence
D. High-dimensional space representations of tokens that the model can interpret and process


6. Why is positional encoding added to embeddings in a transformer model?
A. To give the model information about the sequence order of the tokens
B. To reduce the dimensionality of the embeddings
C. To ensure all vectors have the same length
D. To increase the sentence complexity for better training

